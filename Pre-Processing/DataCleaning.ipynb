{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING AND VISUALIZATION OF RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for analysis and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Set plotting font to Segoe UI (font used for the report)\n",
    "plt.rcParams['font.family'] = 'Segoe UI'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"aircraft_data.csv\")\n",
    "\n",
    "#print(df.shape)       # Shows DataFrame size\n",
    "#print(df.info())      # Shows the size, columns, and data types for each variable\n",
    "\n",
    "\n",
    "# # Represent a heat map for null values\n",
    "\n",
    "# custom_cmap = ListedColormap(['#DAE3F3', '#00205B'])  # Gray for NaNs, Blue for values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(df.notnull(), cbar=False, cmap=custom_cmap)\n",
    "# plt.title(\"Null Values Heat Map\")\n",
    "# plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels by 45 degrees, align to the right\n",
    "# y_labels = range(0, len(df), max(1, len(df) // 12))  # Display ~10 evenly spaced labels\n",
    "# plt.yticks(y_labels, [df.index[i] for i in y_labels], rotation=0)  # Keep y-axis labels horizontal\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERAL DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size before the deletion of repeated rows: (2530, 18)\n",
      "Dataframe size after the deletion of repeated rows: (2527, 18)\n",
      "The size of the Dataset before the deletion of rows with repeated 'S/N': (2527, 18)\n",
      "Valores de 'S/N' que se encontraron repetidos inicialmente: ['36', '1', '210', '42', '62.112', '56', '18253712', '18', '3601', 'L#1558I', '4879', '2013', '24', '2045', '42.AC011', '1481', '1821', '4692179', '3835', '1612', '196', '13276', '4697241', '3257434', '32-0009', '124', '96', '126', '1509', '1500', '1617', '3428', '3236', '42058', 'T20608487', '310R0044', '41568', '20800413', '720', '24-0944', '1976', '3667', '3858', '2017', 'T20609088', '20', '20800402', '1695', '1376', '4745', '62.C021', '75-8233', 'P3351', '1478', '1874', '444/2019', '1917', '1150-2014', '4294', '54641', '2009', '2953', '3287', '705', '892', '924', '700', '4076', '4692023', '4636401', '349', '6', '670', '581', '599', '676', '3167', '41', '50000064', '2059', '1144', '341', '1219', '4697601', '20800309', '53', '52', '1814', '1667', '1709', '4687', '4540', '1922', '2643', '2888', '2660', '1284', '2528', '2539', '1333', '115', '4395', '2915', '2834', '4697598', '750-0101', '4692140', '28R-31261', '8301001', '1811', '255', '680-0177', '11701', '11', '37733', '37514', '421B0877', 'TE-1102', '1138', '4608083', 'TP280', '22355', '46-8408061', 'LA-0193', '1165', '88-13132', '108', '32', '216', '101', '8', '1301', '4867', '340A1201', '42032', '406-0001']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ángela Martín Milán\\AppData\\Local\\Temp\\ipykernel_40288\\2833007324.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filled_rows = non_nan_rows_sn.groupby('S/N').apply(fill_na_with_others).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the Dataset after the deletion of rows with repeated 'S/N': (2380, 18)\n",
      "The size of the Dataset before the deletion of rows with repeated 'REG': (2380, 18)\n",
      "Valores de 'REG' que se encontraron repetidos inicialmente: ['N555HJ', 'OK-WWD04', 'N25646', 'N557BL', 'N571LL', 'G-HEWZ', 'N121SP', 'N74WY', 'G-BXAY', 'N524MR', 'N262MC', 'N125MM', 'N525DE', 'N32V', 'N20GT', 'N686AG', 'N9193Q']\n",
      "The size of the Dataset after the deletion of rows with repeated 'REG': (2363, 18)\n",
      "Columnas con valores únicos (sin contar NaN): ['S/N', 'REG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ángela Martín Milán\\AppData\\Local\\Temp\\ipykernel_40288\\2833007324.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filled_rows = non_nan_rows_sn.groupby('REG').apply(fill_na_with_others).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# DELETION OF VALUES THAT ARE NOT PROVIDING INFORMATION (EG. Not Liste, TBA, etc)\n",
    "df.replace({'Not Listed': np.nan, 'TBA': np.nan, 'tba': np.nan, 'tbc': np.nan, 'None': np.nan,'TBD': np.nan, '-': np.nan, 'n/a': np.nan, 'NA': np.nan, 'TBD  ':np.nan, 'Not Specified':np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "# DELETION OF REPEATED ROWS\n",
    "\n",
    "# Completely repeated\n",
    "print(f'Dataframe size before the deletion of repeated rows: {df.shape}')\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f'Dataframe size after the deletion of repeated rows: {df.shape}')\n",
    "#Conclusion: There were a total of 3 rows which were initially completely repeated \n",
    "\n",
    "\n",
    "\n",
    "# 'S/N' having repeated values, in this case only the row with less NaN is kept, being completed with information from other rows in case of missing values\n",
    "print(f\"The size of the Dataset before the deletion of rows with repeated 'S/N': {df.shape}\")\n",
    "# 1: Separate NaN and values in'S/N' column\n",
    "nan_rows_sn = df[df['S/N'].isna()]\n",
    "non_nan_rows_sn = df[df['S/N'].notna()]\n",
    "\n",
    "# Identify and display repeated 'S/N' values\n",
    "repeated_sn = non_nan_rows_sn['S/N'].value_counts()\n",
    "repeated_sn = repeated_sn[repeated_sn > 1].index.tolist()\n",
    "print(\"Valores de 'S/N' que se encontraron repetidos inicialmente:\", repeated_sn)\n",
    "\n",
    "# 2: Function definition\n",
    "def fill_na_with_others(group):\n",
    "    # Group rows with same 'S/N' and order them starting with the row with less NaN\n",
    "    group = group.loc[group.apply(lambda row: row.isna().sum(), axis=1).sort_values().index]\n",
    "    \n",
    "    # Take the first row (with less NaNs) as base\n",
    "    base_row = group.iloc[0].copy()\n",
    "    \n",
    "    # Fill NaNs in base row using the rest of the rows from the group following its order\n",
    "    for _, row in group.iterrows():\n",
    "        for col in base_row.index:\n",
    "            if pd.isna(base_row[col]) and not pd.isna(row[col]):\n",
    "                base_row[col] = row[col]\n",
    "    \n",
    "    return base_row\n",
    "\n",
    "# 3: Apply the function for each group of 'S/N' and create a DataFrame with the completed rows\n",
    "filled_rows = non_nan_rows_sn.groupby('S/N').apply(fill_na_with_others).reset_index(drop=True)\n",
    "\n",
    "# 4: Join rows with NaNs in 'S/N' and the completed rows\n",
    "df = pd.concat([filled_rows, nan_rows_sn], ignore_index=True)\n",
    "\n",
    "# Check the size of the Dataset after the deletion of rows\n",
    "print(f\"The size of the Dataset after the deletion of rows with repeated 'S/N': {df.shape}\")\n",
    "#Conclusion: There were a total of 147 rows which initially had repeated S/N value\n",
    "\n",
    "\n",
    "\n",
    "# Repeate the same process for REG column\n",
    "print(f\"The size of the Dataset before the deletion of rows with repeated 'REG': {df.shape}\")\n",
    "# 1: Separate NaN and values in'REG' column\n",
    "nan_rows_sn = df[df['REG'].isna()]\n",
    "non_nan_rows_sn = df[df['REG'].notna()]\n",
    "\n",
    "# Identify and display repeated 'REG' values\n",
    "repeated_sn = non_nan_rows_sn['REG'].value_counts()\n",
    "repeated_sn = repeated_sn[repeated_sn > 1].index.tolist()\n",
    "print(\"Valores de 'REG' que se encontraron repetidos inicialmente:\", repeated_sn)\n",
    "\n",
    "# 2: Apply previous defined function for each group of 'REG' and create a DataFrame with the completed rows\n",
    "filled_rows = non_nan_rows_sn.groupby('REG').apply(fill_na_with_others).reset_index(drop=True)\n",
    "\n",
    "# 3: Join rows with NaNs in 'REG' and the completed rows\n",
    "df = pd.concat([filled_rows, nan_rows_sn], ignore_index=True)\n",
    "\n",
    "# Check the size of the Dataset after the deletion of rows\n",
    "print(f\"The size of the Dataset after the deletion of rows with repeated 'REG': {df.shape}\")\n",
    "#Conclusion: There were a total of 17 rows which initially had repeated REG value\n",
    "\n",
    "\n",
    "\n",
    "# DELETION OF NON-RELEVANT COLUMNS \n",
    "\n",
    "# No containing relevant information for the problem to solve: Too soon to konw\n",
    "# #db.drop(columns = '[insert colum]', axis=1, inplace=True)\n",
    "\n",
    "# Having an unique value for all of the rows: Not applicable for the datset under study \n",
    "\n",
    "#Having different values for each of the rows: 'S/N' and 'REG'\n",
    "unique_columns = [col for col in df.columns if df[col].dropna().nunique() == len(df[col].dropna())]\n",
    "print(\"Columnas con valores únicos (sin contar NaN):\", unique_columns)\n",
    "df.drop(columns = unique_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELEMENTARY FEATURE TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDITION\n",
    "\n",
    "# General view\n",
    "# print('Values that the Condition variable takes:')\n",
    "# print(df['Condition'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes 3 values and a high values of NaN.\n",
    "\n",
    "# Ensure this variable is detected as categoric\n",
    "df['Condition'] = df['Condition'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRICE\n",
    "\n",
    "# General view\n",
    "# print('Values that the Price variable takes:')\n",
    "# print(df['Price'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes also the currency before the figure, only numbers will be kept and letters will be extracted into other column\n",
    "# as can be useful for 'Currency'. Commas indicating thousands will be removed. NaN values are present.\n",
    "\n",
    "\n",
    "# Function to keep only numbers\n",
    "def extract_numbers(precio):\n",
    "    if pd.isna(precio):  # Check if the value is NaN\n",
    "        return np.nan\n",
    "    match = re.search(r'[\\d,]+(?:\\.\\d+)?', precio)\n",
    "    if match:\n",
    "        return float(match.group(0).replace(',', ''))\n",
    "    return np.nan  # NaN if no numbers are found\n",
    "\n",
    "# Function to keep only letters previous to the first figure\n",
    "def extract_text(precio):\n",
    "    if pd.isna(precio):  # Check if the value is NaN\n",
    "        return np.nan\n",
    "    if not isinstance(precio, str):  # Ensure that 'precio' is a string\n",
    "         precio = str(precio)  # Convert to string if it's not\n",
    "    match = re.search(r'^(.*?)(\\d)', precio)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None  # None if no text is found\n",
    "\n",
    "# Apply functions\n",
    "df['text'] = df['Price'].apply(extract_text)  # Saved into a new column named 'text'\n",
    "#print(df['text'].value_counts(dropna=False))\n",
    "df['Price'] = df['Price'].apply(extract_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Show resultant DataFrame\n",
    "#print(db['Price'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# Ensure this variable is detected as Float32\n",
    "df['Price'] = df['Price'].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENCY\n",
    "\n",
    "# General view\n",
    "#print('Values that the Currency variable takes:')\n",
    "# print(df['Currency'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes a high number of NaN values, NaN will be completed from the currency specified into the Price column\n",
    "\n",
    "# Ensure this variable is detected as categoric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Engines\n",
      "1       1312\n",
      "<NA>     744\n",
      "2        307\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY\n",
    "\n",
    "# General view\n",
    "# print('Values that the Category variable takes:')\n",
    "# print(df['Category'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes 13 different values with no NaN, some of them are referring to the same category of aircraft but are \n",
    "# diferently written (some typos as well). As well they are not representing the same aircraft classification, so additional columns will be \n",
    "# created to ensure the same level of information.\n",
    "\n",
    "# Firstly let's group values that are referring to the same concept\n",
    "df['Category'] = df['Category'].str.replace('Single Piston','Single Engine Piston', regex=False)         # Single Piston means the same as Single Engine Piston\n",
    "df['Category'] = df['Category'].str.replace('Twin Piston','Multi Engine Piston', regex=False)            # Twin Piston means the same as Multi Engine Piston\n",
    "df['Category'] = df['Category'].str.replace('Turboprops','Turboprop', regex=False)                       # Turboprops means the same as Turboprop\n",
    "df['Category'] = df['Category'].str.replace('Gliders | Sailplanes','Gliders/Sailplanes', regex=False)    # Change  '|' separation mark by '/'\n",
    "\n",
    "\n",
    "# Fix detected errors\n",
    "# The Boeing PT-17 model is labeled as Private Jet, but it's actually a vintage military aircraft\n",
    "df.loc[df['Model'] == 'PT-17', ['Category']] = 'Military/Classic/Vintage'\n",
    "\n",
    "\n",
    "# From this first Category column, three more columns will be added representing the same level of classification: Category, Propulsion and No. Engines\n",
    "\n",
    "# Aircraft Category\n",
    "df.loc[df['Category'].isin(['Single Engine Piston', 'Multi Engine Piston', 'Private Jets', 'Turboprop']), 'category'] = 'Airplane'\n",
    "df.loc[df['Category'].isin(['Turbine Helicopters', 'Piston Helicopters', 'Gyrocopter']), 'category'] = 'Rotorcraft'\n",
    "df.loc[df['Category'].isin(['Military/Classic/Vintage']), 'category'] = 'Military/Classic/Vintage'\n",
    "df.loc[df['Category'].isin(['Ultralight']), 'category'] = 'Ultralight'\n",
    "df.loc[df['Category'].isin(['Gliders/Sailplanes']), 'category'] = 'Motor Gliders/Sailplanes' # All are motor gliders\n",
    "\n",
    "#print(df['category'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "#Aircraft propulsion:\n",
    "df.loc[df['Category'].isin(['Single Engine Piston', 'Multi Engine Piston']), 'Propulsion'] = 'Piston Engine'\n",
    "df.loc[df['Category'].isin(['Private Jets']), 'Propulsion'] = 'Turbofan'\n",
    "df.loc[df['Category'].isin(['Turboprop']), 'Propulsion'] = 'Turboprop'\n",
    "df.loc[df['Category'].isin(['Turbine Helicopters']), 'Propulsion'] = 'Turboshaft' \n",
    "df.loc[df['Category'].isin(['Piston Helicopters']), 'Propulsion'] = 'Piston Engine'\n",
    "df.loc[df['Category'].isin(['Gyrocopter']), 'Propulsion'] = 'Gyrocopter'\n",
    "df.loc[df['Category'].isin(['Military/Classic/Vintage']), 'Propulsion'] = np.nan\n",
    "df.loc[df['Category'].isin(['Ultralight']), 'Propulsion'] = np.nan\n",
    "df.loc[df['Category'].isin(['Gliders/Sailplanes']), 'Propulsion'] = np.nan # Piston engines\n",
    "\n",
    "#print(df['Propulsion'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# No. Engines - ASK\n",
    "df.loc[df['Category'].isin(['Single Engine Piston', 'Gyrocopter']), 'No. Engines'] = 1\n",
    "df.loc[df['Category'].isin(['Multi Engine Piston']), 'No. Engines'] = 2\n",
    "\n",
    "#print(df['No. Engines'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# Ensure these columns are detected as categoric and integer32\n",
    "df['category'] = df['category'].astype('category')\n",
    "df['Propulsion'] = df['Propulsion'].astype('category')\n",
    "df['No. Engines'] = df['No. Engines'].astype('Int32')\n",
    "\n",
    "print(df['No. Engines'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR\n",
    "\n",
    "# General view\n",
    "# print('Values that the Year variable takes:')\n",
    "# print(df['Year'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes many different values with no NaN, this variable is identified as categoric, it will be converted into \n",
    "# numeric (integer)\n",
    "\n",
    "\n",
    "# Convert from categoric to numeric values (integer): Int32\n",
    "df['Year'] = df['Year'].astype('Int32')\n",
    "\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE\n",
    "\n",
    "# General view\n",
    "# print('Values that the Make variable takes:')\n",
    "# print(df['Make'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes many different values with no NaN, the number of different values is too high that they can not be checked\n",
    "# just format improvements will be made\n",
    "\n",
    "# Unify value formats by using uppercase for only the first letter of each word\n",
    "df['Make'] = df['Make'].str.title()\n",
    "# Change to data type to String\n",
    "df[\"Make\"] = df[\"Make\"].astype(\"string\")\n",
    "\n",
    "#print(df['Make'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "# General view\n",
    "# print('Values that the Model variable takes:')\n",
    "# print(df['Model'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes many different values with no NaN, the number of different values is too high that they can not be checked\n",
    "# just format improvements will be made\n",
    "\n",
    "\n",
    "# Unify value formats by using uppercase for only the first letter of each word\n",
    "df['Model'] = df['Model'].str.title()\n",
    "# Change to data type to String\n",
    "df[\"Model\"] = df[\"Model\"].astype(\"string\")\n",
    "\n",
    "# #Para comprobar:\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#print(db['Model'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATION\n",
    "\n",
    "# General view\n",
    "# print('Values that the Location variable takes:')\n",
    "# print(df['Location'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes many different values and NaN. In general it can be observed that there exists three different types in which\n",
    "# Location is reported, and normally the city, state and country are provided. Due to this, four additional columns will be added: 'City',\n",
    "# 'State', 'Country' and 'Continent'\n",
    "\n",
    "# Function to extract city, state, country, and continent depending on the format in which Location is expressed\n",
    "def extract_location(location):\n",
    "    # Check that the cell is a string\n",
    "    if isinstance(location, str):\n",
    "        # Split the cell into lines\n",
    "        lines = location.split('\\n')\n",
    "        \n",
    "        # Case 1: Three lines, format city-state-country\n",
    "        if len(lines) == 3:\n",
    "            city = lines[0].replace(',', '').strip()\n",
    "            state = re.sub(r'\\s+', '', lines[1]) if lines[1].strip() else np.nan\n",
    "            country = re.sub(r'\\s+', '', lines[2])\n",
    "            continent = np.nan\n",
    "            return pd.Series([city, state, country, continent])\n",
    "\n",
    "        # Case 2: Two lines, format state-country\n",
    "        elif len(lines) == 2:\n",
    "            city = np.nan\n",
    "            state = re.sub(r'\\s+', '', lines[0])  # Remove spaces in state\n",
    "            country = re.sub(r'\\s+', '', lines[1])  # Remove spaces in country\n",
    "            continent = np.nan\n",
    "            return pd.Series([city, state, country, continent])\n",
    "        \n",
    "        # Case 3: One line with commas, format continent-country-state\n",
    "        elif len(lines) == 1 and ',' in lines[0]:\n",
    "            # Split the line into parts using commas\n",
    "            parts = lines[0].split(',')\n",
    "            \n",
    "            # Extract continent (text before the first comma)\n",
    "            continent = parts[0].strip()\n",
    "            \n",
    "            # Extract country and state (if exists)\n",
    "            if len(parts) > 1:\n",
    "                country_and_state = parts[1].strip()\n",
    "                \n",
    "                # Check if there's a hyphen to split country and state\n",
    "                if '-' in country_and_state:\n",
    "                    country, state = map(str.strip, country_and_state.split('-', 1))\n",
    "                else:\n",
    "                    country = country_and_state\n",
    "                    state = np.nan\n",
    "            else:\n",
    "                country = np.nan\n",
    "                state = np.nan\n",
    "            \n",
    "            # City doesn't apply in this format\n",
    "            city = np.nan\n",
    "            return pd.Series([city, state, country, continent])\n",
    "    \n",
    "    # Return NaN for each field if format is invalid\n",
    "    return pd.Series([np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "# Apply the function and create new columns\n",
    "df[['City', 'State', 'Country', 'Continent']] = df['Location'].apply(extract_location)\n",
    "\n",
    "\n",
    "# Function to verify if a cell matches any of these formats to get values not presenting the three formats described above\n",
    "def check_location_format(location):\n",
    "    if isinstance(location, str):\n",
    "        lines = location.split('\\n')\n",
    "        \n",
    "        # Check the first format: three lines\n",
    "        if len(lines) == 3:\n",
    "            city = lines[0].replace(',', '').strip()\n",
    "            state = re.sub(r'\\s+', '', lines[1]) if lines[1].strip() else \"\"\n",
    "            country = re.sub(r'\\s+', '', lines[2])\n",
    "            if city and country:\n",
    "                return True  # Matches the three-line format\n",
    "            \n",
    "\n",
    "        # Check the second format: two lines, state-country\n",
    "        elif len(lines) == 2:\n",
    "            state = re.sub(r'\\s+', '', lines[0])\n",
    "            country = re.sub(r'\\s+', '', lines[1])\n",
    "            if state and country:\n",
    "                return True  # Matches the two-line format\n",
    "            \n",
    "\n",
    "        # Check the third format: one line with commas\n",
    "        elif len(lines) == 1 and ',' in lines[0]:\n",
    "            parts = lines[0].split(',')\n",
    "            continent = parts[0].strip()\n",
    "            if len(parts) > 1:\n",
    "                country_and_state = parts[1].strip()\n",
    "                if '-' in country_and_state or country_and_state:\n",
    "                    return True  # Matches the one-line with commas format\n",
    "    \n",
    "    # Doesn't match any of the formats\n",
    "    return False\n",
    "\n",
    "# Filter rows that don't match the formats\n",
    "invalid_locations = df[~df['Location'].apply(check_location_format)]\n",
    "\n",
    "# Display results\n",
    "# print(\"Cells that don't match any of the formats:\")\n",
    "# print(invalid_locations[['Location']])\n",
    "\n",
    "# Fill cells that don't match any of the three described formats\n",
    "df.loc[df['Location'] == 'DEU', ['Country']] = 'Germany'\n",
    "df.loc[df['Location'] == 'ESP', ['Country']] = 'Spain'\n",
    "df.loc[df['Location'] == 'FRA', ['Country']] = 'France'\n",
    "df.loc[df['Location'] == 'CHE', ['Country']] = 'Switzerland'\n",
    "df.loc[df['Location'] == 'ITA', ['Country']] = 'Italy'\n",
    "df.loc[df['Location'] == 'AUS', ['Country']] = 'Australia'\n",
    "df.loc[df['Location'] == 'CAN', ['Country']] = 'Canada'\n",
    "df.loc[df['Location'] == 'CHL', ['Country']] = 'Chile'\n",
    "df.loc[df['Location'] == 'USA', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Europe', ['Continent']] = 'Europe'\n",
    "df.loc[df['Location'] == 'Asia', ['Continent']] = 'Asia'\n",
    "df.loc[df['Location'] == 'Temple', ['City']] = 'Temple'\n",
    "df.loc[df['Location'] == 'Temple', ['State']] = 'CA'\n",
    "df.loc[df['Location'] == 'Temple', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Bethany', ['City']] = 'Bethany'\n",
    "df.loc[df['Location'] == 'Bethany', ['State']] = 'OK'\n",
    "df.loc[df['Location'] == 'Bethany', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Nashville', ['City']] = 'Nashville'\n",
    "df.loc[df['Location'] == 'Nashville', ['State']] = 'TN'\n",
    "df.loc[df['Location'] == 'Nashville', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Madison', ['City']] = 'Madison'\n",
    "df.loc[df['Location'] == 'Madison', ['State']] = 'WI'\n",
    "df.loc[df['Location'] == 'Madison', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Ft. Lauderdale', ['City']] = 'Ft. Lauderdale'\n",
    "df.loc[df['Location'] == 'Ft. Lauderdale', ['State']] = 'FL'\n",
    "df.loc[df['Location'] == 'Ft. Lauderdale', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'KCHD', ['City']] = 'Chandler'\n",
    "df.loc[df['Location'] == 'KCHD', ['State']] = 'AZ'\n",
    "df.loc[df['Location'] == 'KCHD', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Newnan', ['City']] = 'Newnan'\n",
    "df.loc[df['Location'] == 'Newnan', ['State']] = 'GA'\n",
    "df.loc[df['Location'] == 'KNewnan', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Corvallis', ['City']] = 'Corvallis'\n",
    "df.loc[df['Location'] == 'Corvallis', ['State']] = 'OR'\n",
    "df.loc[df['Location'] == 'Corvallis', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Cameron', ['City']] = 'Cameron'\n",
    "df.loc[df['Location'] == 'Cameron', ['State']] = 'TX'\n",
    "df.loc[df['Location'] == 'Cameron', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Heber City', ['City']] = 'Heber City'\n",
    "df.loc[df['Location'] == 'Heber City', ['State']] = 'UT'\n",
    "df.loc[df['Location'] == 'Heber City', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == '73501', ['City']] = 'Lawton'\n",
    "df.loc[df['Location'] == '73501', ['State']] = 'OK'\n",
    "df.loc[df['Location'] == '73501', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Jacksonville - CRG', ['City']] = 'Jacksonville'\n",
    "df.loc[df['Location'] == 'Jacksonville - CRG', ['State']] = 'FL'\n",
    "df.loc[df['Location'] == 'Jacksonville - CRG', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Jacksonville', ['City']] = 'Jacksonville'\n",
    "df.loc[df['Location'] == 'Jacksonville', ['State']] = 'FL'\n",
    "df.loc[df['Location'] == 'Jacksonville', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Alexandria', ['City']] = 'Alexandria'\n",
    "df.loc[df['Location'] == 'Alexandria', ['State']] = 'MN'\n",
    "df.loc[df['Location'] == 'Alexandria', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Lodi', ['City']] = 'Lodi'\n",
    "df.loc[df['Location'] == 'Lodi', ['State']] = 'CA'\n",
    "df.loc[df['Location'] == 'Lodi', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Spokane', ['City']] = 'Spokane'\n",
    "df.loc[df['Location'] == 'Spokane', ['State']] = 'WA'\n",
    "df.loc[df['Location'] == 'Spokane', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'North America + Canada', ['Continent']] = 'North America'\n",
    "df.loc[df['Location'] == 'AUBURN', ['City']] = 'Auburn'\n",
    "df.loc[df['Location'] == 'AUBURN', ['State']] = 'AL'\n",
    "df.loc[df['Location'] == 'AUBURN', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Kankakee', ['City']] = 'Kankakee'\n",
    "df.loc[df['Location'] == 'Kankakee', ['State']] = 'IL'\n",
    "df.loc[df['Location'] == 'Kankakee', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Oxford (KHNZ)', ['City']] = 'Oxford'\n",
    "df.loc[df['Location'] == 'Oxford (KHNZ)', ['State']] = 'NC'\n",
    "df.loc[df['Location'] == 'Oxford (KHNZ)', ['Country']] = 'USA'\n",
    "df.loc[df['Location'] == 'Madras', ['City']] = 'Madras'\n",
    "df.loc[df['Location'] == 'Madras', ['Country']] = 'India'\n",
    "df.loc[df['Location'] == 'Madras', ['Continent']] = 'Asia'\n",
    "df.loc[df['Location'] == 'Durango', ['City']] = 'Durango'\n",
    "df.loc[df['Location'] == 'Durango', ['Country']] = 'Spain'\n",
    "df.loc[df['Category'].isin(['DEU', 'ESP', 'FRA', 'CHE', 'ITA']), 'Continent'] = 'Europe'\n",
    "df.loc[df['Category'].isin(['AUS']), 'Continent'] = 'Oceania'\n",
    "df.loc[df['Category'].isin(['CHL']), 'Continent'] = 'South America'\n",
    "\n",
    "# Standardize some cell values for consistency\n",
    "df['Country'] = df['Country'].str.replace('England', 'United Kingdom', regex=False)\n",
    "df['Continent'] = df['Continent'].str.replace('Australia & NZ', 'Oceania', regex=False)\n",
    "df['Continent'] = df['Continent'].str.replace('North America + Canada', 'North America', regex=False)\n",
    "df.loc[df['City'] == 'Canada', ['City']] = np.nan\n",
    "df.loc[df['Location'] == 'Canada', ['Country']] = 'Canada'\n",
    "df.loc[df['Country'] == 'USA', ['Country']] = 'United States'\n",
    "df.loc[df['Country'] == 'MEX', ['Country']] = 'Mexico'\n",
    "df.loc[df['Country'] == 'AUS', ['Country']] = 'Australia'\n",
    "df.loc[df['Country'] == 'CAN', ['Country']] = 'Canada'\n",
    "df.loc[df['Country'] == 'CHE', ['Country']] = 'Switzerland'\n",
    "df.loc[df['Country'] == 'ZAF', ['Country']] = 'South Africa'\n",
    "df.loc[df['Country'] == 'BEL', ['Country']] = 'Belgium'\n",
    "df.loc[df['Country'] == 'POL', ['Country']] = 'Poland'\n",
    "df.loc[df['Country'] == 'NGA', ['Country']] = 'Nigeria'\n",
    "df.loc[df['Country'] == 'GBR', ['Country']] = 'United Kingdom'\n",
    "df.loc[df['Country'] == 'SRB', ['Country']] = 'Serbia'\n",
    "df.loc[df['Country'] == 'NLD', ['Country']] = 'Netherlands'\n",
    "df.loc[df['Country'] == 'CZE', ['Country']] = 'Czech Republic'\n",
    "df.loc[df['Country'] == 'ARG', ['Country']] = 'Argentina'\n",
    "df.loc[df['Country'] == 'FRA', ['Country']] = 'France'\n",
    "df.loc[df['Country'] == 'URY', ['Country']] = 'Uruguay'\n",
    "df.loc[df['Country'] == 'KEN', ['Country']] = 'Kenya'\n",
    "df.loc[df['Continent'] == 'Middle East', ['Continent']] = 'Asia'\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "#print(df[['Location', 'City', 'State', 'Country', 'Continent']])\n",
    "\n",
    "\n",
    "#Ensure correct data type for each of the columns\n",
    "df[\"City\"] = df[\"City\"].astype(\"string\")\n",
    "df[\"State\"] = df[\"State\"].astype(\"category\")\n",
    "df[\"Country\"] = df[\"Country\"].astype(\"category\")\n",
    "df[\"Continent\"] = df[\"Continent\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL HOURS\n",
    "\n",
    "# General view\n",
    "# print('Values that the Total Hours variable takes:')\n",
    "# print(df['Total Hours'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes many different values and NaN. Also the format needs to be aligned  (some values have commas for\n",
    "# representing thousands, others have a hh:mm:ss format, some words are included, etc)\n",
    "\n",
    "# Function for removing thousands commas and convert the last comma into a decimal separator ('.')\n",
    "def process_commas(value):\n",
    "    value = str(value)  # Standardize data format in the column\n",
    "    if value.count(',') > 1:\n",
    "        # If there's more than one comma, treat the last comma as a decimal separator\n",
    "        parts = value.rsplit(',', 1)\n",
    "        processed_value = parts[0].replace(',', '') + '.' + parts[1]\n",
    "    else:\n",
    "        # If only one comma, remove thousands comma\n",
    "        processed_value = value.replace(',', '')\n",
    "    return processed_value\n",
    "\n",
    "# Apply previous function\n",
    "df['Total Hours'] = df['Total Hours'].apply(process_commas)\n",
    "\n",
    "\n",
    "# Function to convert hh:mm:ss to decimal hours\n",
    "def convert_hours(value):\n",
    "    value = str(value) # Standardize data format in the column\n",
    "    try:\n",
    "        # Split hours, minutes, and seconds\n",
    "        hours, minutes, seconds = map(int, value.split(':'))\n",
    "        # Convert minutes to fraction of hours (ignoring seconds)\n",
    "        decimal_hours = hours + minutes / 60\n",
    "        return round(decimal_hours, 2)  # Round to 2 decimal places\n",
    "    except:\n",
    "        # Return original value if it doesn’t match the expected format\n",
    "        return value\n",
    "\n",
    "# Apply previous function\n",
    "df['Total Hours'] = df['Total Hours'].apply(convert_hours)\n",
    "\n",
    "\n",
    "# Remove words\n",
    "words_to_remove = ['SMOH', 'SFRM', 'hrs', 'hours', 'approx', 'SNEW', 'HOURS', 'Hours', 'Flight Hours (approx.)', 'since new', 'h', '*', 'Delivery time only']\n",
    "regex_pattern = '|'.join(map(re.escape, words_to_remove))\n",
    "df['Total Hours'] = df['Total Hours'].str.replace(regex_pattern, '', regex=True).str.strip()\n",
    "\n",
    "# Resolve specific cases: 2.518:53, 3181:57, 10 8619, 12 Yrs / 2200 Hrs Remaining, Sub 870  TTSN 1883tt 145 since O/H\n",
    "df.loc[df['Total Hours'] == '2.518:53', ['Total Hours']] = 2518.88\n",
    "df.loc[df['Total Hours'] == '3181:57', ['Total Hours']] = 3181.95\n",
    "df.loc[df['Total Hours'] == '10 8619', ['Total Hours']] = 10861.9\n",
    "df.loc[df['Total Hours'] == '12 Yrs / 2200 Hrs Remaining', ['Total Hours']] = np.nan\n",
    "df.loc[df['Total Hours'] == 'Sub 870  TTSN', ['Total Hours']] = 870\n",
    "df.loc[df['Total Hours'] == '1883tt  145 since O/H', ['Total Hours']] = 145\n",
    "df.loc[df['Total Hours'] == '', ['Total Hours']] = np.nan\n",
    "\n",
    "\n",
    "# Check: Display values that do NOT contain only numbers (excluding valid decimals, i.e., numbers with only one dot)\n",
    "non_numeric_values = df['Total Hours'][~df['Total Hours'].str.match(r'^\\d+(\\.\\d+)?$', na=False)]\n",
    "#print(non_numeric_values.values)\n",
    "\n",
    "# Verification\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#print(df['Total Hours'].value_counts(dropna=False))\n",
    "\n",
    "# Convert the column to float\n",
    "df['Total Hours'] = df['Total Hours'].astype('float32')\n",
    "\n",
    "\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL SEATS\n",
    "\n",
    "# General view\n",
    "# print('Values that the Total Seats variable takes:')\n",
    "#print(df['Total Seats'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes 15 values and many NaNs, convert to integer\n",
    "\n",
    "# Convert to numeric values (integer): Int64\n",
    "df['Total Seats'] = df['Total Seats'].astype('Int32')\n",
    "\n",
    "#print(df.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLIGHT RULES\n",
    "\n",
    "# General view\n",
    "# print('Values that the FLight Rules variable takes:')\n",
    "#print(df['Flight Rules'].value_counts(dropna=False))\n",
    "# Main remarks: This variable includes only 3 values and many NaNs, these three values can be converted to intergers to save memory?\n",
    "# 1: IFR, 2:VFR, 3:BTH\n",
    "\n",
    "# df['Flight Rules'].replace({'IFR': '1', 'VFR': '2', 'BTH' : '3'}, inplace=True)\n",
    "# df['Flight Rules']=pd.to_numeric(df['Flight Rules'], downcast='integer', errors='coerce')\n",
    "# df['Flight Rules'] = df['Flight Rules'].astype('Int64')\n",
    "\n",
    "# BTH is the same as IFR\n",
    "df.loc[df['Flight Rules'] == 'BTH', ['Flight Rules']] = 'IFR'\n",
    "\n",
    "#Convert to correct data type\n",
    "df['Flight Rules'] = df['Flight Rules'].astype('category')\n",
    "\n",
    "#print(df['Flight Rules'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINE 1 HOURS, ENGINE 2 HOURS, PROPELLER 1 HOURS, PROPELLER 2 HOURS\n",
    "\n",
    "# General view\n",
    "# print('Values that the Engine 1 Hours variable takes:')\n",
    "# print(df['Engine 1 Hours'].value_counts(dropna=False))\n",
    "# Main remarks: Many NaN values, come values include: 'hours', 'SNEW' = since new, 'SMOH' = since major overhaul, 'SPOH' = since propeller \n",
    "# overhaul, 'SCMOH' = Since Chrome Major Overhaul, separate numbers and letters and convert numbers to integers. Columna casi sin datos. \n",
    "# If it is not a missing value but instead it is not applicable use 'N/A'\n",
    "\n",
    "\n",
    "\n",
    "# Separate maintenance type\n",
    "# Function to extract numbers and letters\n",
    "def separate_numbers_letters(value):\n",
    "    if pd.isna(value):  # Check if the value is NaN\n",
    "        return np.nan, np.nan\n",
    "    numbers = ''.join(filter(str.isdigit, value))  # Extracts the numbers\n",
    "    letters = ''.join(filter(str.isalpha, value))  # Extracts the letters\n",
    "    return numbers, letters\n",
    "\n",
    "# Apply the function and assign the results to new columns\n",
    "df[['Eng. 1 Hours', 'Maint. Eng. 1']] = df['Engine 1 Hours'].apply(separate_numbers_letters).apply(pd.Series)\n",
    "df[['Eng. 2 Hours', 'Maint. Eng. 2']] = df['Engine 2 Hours'].apply(separate_numbers_letters).apply(pd.Series)\n",
    "df[['Prop. 1 Hours', 'Maint. Prop. 1']] = df['Prop 1 Hours'].apply(separate_numbers_letters).apply(pd.Series)\n",
    "df[['Prop. 2 Hours', 'Maint. Prop. 2']] = df['Prop 2 Hours'].apply(separate_numbers_letters).apply(pd.Series)\n",
    "\n",
    "\n",
    "# Solve values inconsistencies in Maint. columns:\n",
    "\n",
    "# MAINT. ENG. 1\n",
    "#print(df['Maint. Eng. 1'].value_counts(dropna=False))\n",
    "# Convert \"HOURS\" and \"\" values into NaN in 'Maint. Eng. 1' column\n",
    "df['Maint. Eng. 1'] = df['Maint. Eng. 1'].replace(['HOURS', ''], np.nan)\n",
    "# Unifying TSOH = Time Since Overhaul and SOH = Since Overhaul\n",
    "df.loc[df['Maint. Eng. 1'] == 'TSOH', ['Maint. Eng. 1']] = 'SOH'\n",
    "# Remove T = Time in TSHSI to homogenize to the rest of the values and add T to CZI and MPI\n",
    "df.loc[df['Maint. Eng. 1'] == 'TSHSI', ['Maint. Eng. 1']] = 'SHSI'\n",
    "df.loc[df['Maint. Eng. 1'] == 'CZI', ['Maint. Eng. 1']] = 'SCZI'\n",
    "df.loc[df['Maint. Eng. 1'] == 'MPI', ['Maint. Eng. 1']] = 'SMPI'\n",
    "# TSHS with SHSI to homogenize values\n",
    "df.loc[df['Maint. Eng. 1'] == 'TSHS', ['Maint. Eng. 1']] = 'SHSI'\n",
    "#print(df['Maint. Eng. 1'].value_counts(dropna=False))\n",
    "\n",
    "# MAINT. ENG. 2\n",
    "#print(df['Maint. Eng. 2'].value_counts(dropna=False))\n",
    "# Same changes as for Maint. Eng. 1:\n",
    "df['Maint. Eng. 2'] = df['Maint. Eng. 2'].replace(['HOURS', ''], np.nan)\n",
    "df.loc[df['Maint. Eng. 2'] == 'CZI', ['Maint. Eng. 2']] = 'SCZI'\n",
    "df.loc[df['Maint. Eng. 2'] == 'TSHSI', ['Maint. Eng. 2']] = 'SHSI'\n",
    "df.loc[df['Maint. Eng. 2'] == 'TSOH', ['Maint. Eng. 2']] = 'SOH'\n",
    "df.loc[df['Maint. Eng. 2'] == 'MPI', ['Maint. Eng. 2']] = 'SMPI'\n",
    "df.loc[df['Maint. Eng. 2'] == 'TSHS', ['Maint. Eng. 2']] = 'SHSI'\n",
    "#print(df['Maint. Eng. 2'].value_counts(dropna=False))\n",
    "\n",
    "# MAINT. PROP. 1\n",
    "# print(df['Maint. Prop. 1'].value_counts(dropna=False))\n",
    "# Same changes as for the rest of Maint.:\n",
    "df['Maint. Prop. 1'] = df['Maint. Prop. 1'].replace(['HOURS', ''], np.nan)\n",
    "#print(df['Maint. Prop. 1'].value_counts(dropna=False))\n",
    "\n",
    "# MAINT. PROP. 2\n",
    "#print(df['Maint. Prop. 2'].value_counts(dropna=False))\n",
    "# Same changes as for the rest of Maint.:\n",
    "df['Maint. Prop. 2'] = df['Maint. Prop. 2'].replace(['HOURS', ''], np.nan)\n",
    "df.loc[df['Maint. Prop. 2'] == 'TSHS', ['Maint. Prop. 2']] = 'SHSI'\n",
    "#print(df['Maint. Prop. 2'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "# Verify the result\n",
    "# print(df[['Engine 1 Hours', 'Eng. 1 Hours', 'Maint. Eng. 1']])\n",
    "# print(df[['Engine 2 Hours', 'Engine 2 hours', 'Maint. Eng. 2']])\n",
    "# print(df[['Prop 1 Hours', 'Prop 1 hours', 'Maint. Prop. 1']])\n",
    "# print(df[['Prop 2 Hours', 'Prop 2 hours', 'Maint. Prop. 2']])\n",
    "\n",
    "\n",
    "# Convert the columns to numeric values and category\n",
    "df['Eng. 1 Hours'] = df['Eng. 1 Hours'].astype('float32')\n",
    "df['Eng. 2 Hours'] = df['Eng. 2 Hours'].astype('float32')\n",
    "df['Prop. 1 Hours'] = df['Prop. 2 Hours'].astype('float32')\n",
    "df['Prop. 2 Hours'] = df['Prop. 2 Hours'].astype('float32')\n",
    "df['Maint. Eng. 1'] = df['Maint. Eng. 1'].astype('category')\n",
    "df['Maint. Eng. 2'] = df['Maint. Eng. 2'].astype('category')\n",
    "df['Maint. Prop. 1'] = df['Maint. Prop. 1'].astype('category')\n",
    "df['Maint. Prop. 2'] = df['Maint. Prop. 2'].astype('category')\n",
    "\n",
    "#print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ángela Martín Milán\\AppData\\Local\\Temp\\ipykernel_40288\\2268868285.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['National Origin'].replace({'Eu': 'EU', 'Swtizerland': 'Switzerland', 'Czechoslovakia' : 'Czech Republic', 'Britain':'United Kingdom'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# NATIONAL ORIGIN \n",
    "\n",
    "# General view\n",
    "# print('Values that the National Origin variable takes:')\n",
    "# print(df['National Origin'].value_counts(dropna=False))\n",
    "# Main remarks: 26 values and some NULLs, some with uppercase/lowercase differences; convert to lowercase to avoid duplicates, e.g., italy\n",
    "# EU replace with United States, Switzerland misspelled, Czechoslovakia = Slovakia, Britain = United Kingdom, France and Britain counted twice\n",
    "\n",
    "\n",
    "# Unify value formats by using uppercase for only the first letter of each word\n",
    "df['National Origin'] = df['National Origin'].str.title()\n",
    "\n",
    "# Ammend values\n",
    "df['National Origin'].replace({'Eu': 'EU', 'Swtizerland': 'Switzerland', 'Czechoslovakia' : 'Czech Republic', 'Britain':'United Kingdom'}, inplace=True)\n",
    "\n",
    "\n",
    "# The same degree of precision is not achieved by EU value, as there are 59 EU values, the model are going to be studied to ammend the specific country in which they \n",
    "# are produced\n",
    "\n",
    "# Filter rows with EU into National Origin column\n",
    "df_filtered = df[df['National Origin'] == 'EU']\n",
    "# Show 'Model' and 'Make' for those rows\n",
    "#print(df_filtered[['Model', 'Make', 'National Origin']])\n",
    "\n",
    "# Ammend values\n",
    "df.loc[df['Model'] == 'As 350Ba', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Bk 117 B-2', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'Ec 135P2', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'As 350B-2', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 120B', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 365N-1', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 355F-2', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 355N', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 135', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'Ec 120', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'H125', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'H130', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 350B-3', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 130B4', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'H145', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'Ec 145', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'Ec 135P2+', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'As 355Np', ['National Origin']] = 'Germany'\n",
    "df.loc[df['Model'] == 'As 350B', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 225', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 365N-2', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'A320', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 155B1', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Ec 155B', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 355F-2', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'As 350Sd2', ['National Origin']] = 'France'\n",
    "df.loc[df['Model'] == 'Bo 105Cbs-4', ['National Origin']] = 'Germany'\n",
    "\n",
    "# Convert the columns to category\n",
    "df['National Origin'] = df['National Origin'].astype('category')\n",
    "\n",
    "#print(df['National Origin'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove no longer necessary columns \n",
    "# print(df.columns)\n",
    "df.drop(columns = ['Category', 'Engine 1 Hours', 'Engine 2 Hours', 'Prop 1 Hours', 'Prop 2 Hours', 'Location'], axis=1, inplace=True)\n",
    "# print(df.columns)\n",
    "\n",
    "# Rename reamining ones (only required for Category)\n",
    "df.rename(columns={'category': 'Category'}, inplace=True)\n",
    "# print(df.columns)\n",
    "\n",
    "# Reorder columns\n",
    "new_order = ['Price', 'Currency', 'Year', 'Condition', 'Total Hours', 'Total Seats','Flight Rules', 'Category', 'Propulsion',\n",
    "             'No. Engines', 'Make', 'Model', 'City', \n",
    "             'State', 'Country', 'Continent', 'National Origin', 'Eng. 1 Hours', 'Maint. Eng. 1',\n",
    "             'Eng. 2 Hours', 'Maint. Eng. 2', 'Prop. 1 Hours', 'Maint. Prop. 1',\n",
    "             'Prop. 2 Hours', 'Maint. Prop. 2', 'text']\n",
    "df = df[new_order]\n",
    "\n",
    "#print(df.info())\n",
    "\n",
    "\n",
    "# Save the clean DataFrame linto a CSV file\n",
    "df.to_csv(r'C:\\Users\\Ángela Martín Milán\\Desktop\\TFM\\clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
